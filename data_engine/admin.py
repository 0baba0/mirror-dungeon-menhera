import os
import json
import time
import shutil
import hashlib
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from flask import Flask, render_template, request, redirect, url_for, send_from_directory

app = Flask(__name__)

# --- ê²½ë¡œ ì„¤ì • ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
WEB_SITE_DIR = os.path.abspath(os.path.join(BASE_DIR, "..", "web_site"))
IMAGE_DIR = os.path.join(WEB_SITE_DIR, "public", "images", "characters")
TEMP_DIR = os.path.join(WEB_SITE_DIR, "public", "images", "temp")
JSON_DIR = os.path.join(WEB_SITE_DIR, "src", "content", "characters")

os.makedirs(TEMP_DIR, exist_ok=True)
os.makedirs(IMAGE_DIR, exist_ok=True)
os.makedirs(JSON_DIR, exist_ok=True)

# íŒŒì¼ ì§€ë¬¸(MD5 Hash) ì¶”ì¶œ í—¬í¼ í•¨ìˆ˜
def get_file_hash(filepath):
    hasher = hashlib.md5()
    with open(filepath, 'rb') as f:
        hasher.update(f.read())
    return hasher.hexdigest()

# ==========================================
# 1. ê³µí†µ ì´ë¯¸ì§€ ì„œë¹™ ë¼ìš°í„°
# ==========================================
@app.route('/images/<filename>')
def serve_image(filename):
    return send_from_directory(IMAGE_DIR, filename)

@app.route('/temp_images/<filename>')
def serve_temp_image(filename):
    return send_from_directory(TEMP_DIR, filename)

# ==========================================
# 2. ë„ê° ë°ì´í„° íŒ©í† ë¦¬ (ë©”ì¸ í™”ë©´)
# ==========================================
@app.route('/')
def index():
    images = sorted([f for f in os.listdir(IMAGE_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])
    
    # URLì— page ë²ˆí˜¸ê°€ ì—†ì„ ë•Œ (ìŠ¤ë§ˆíŠ¸ ì´ì–´í•˜ê¸°)
    if 'page' not in request.args:
        target_page = len(images) 
        for i, img in enumerate(images):
            char_id = os.path.splitext(img)[0]
            if not os.path.exists(os.path.join(JSON_DIR, f"{char_id}.json")):
                target_page = i
                break
        return redirect(url_for('index', page=target_page))

    # ğŸš€ ë¹ˆì¹¸ì´ë‚˜ ì´ìƒí•œ ë¬¸ìê°€ ë“¤ì–´ì˜¤ë©´ ë¬´ì¡°ê±´ 0(ì²˜ìŒ)ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ë°©ì–´ ë¡œì§
    page_str = request.args.get('page', '0')
    page = int(page_str) if page_str.isdigit() else 0
    if page < 0: page = 0
    
    # ğŸš€ ì‹ ê·œ: ë“œë¡­ë‹¤ìš´ ê²€ìƒ‰ì„ ìœ„í•œ ì „ì²´ ë°ì´í„° ëª©ë¡ ìƒì„±
    search_list = []
    for i, img in enumerate(images):
        char_id = os.path.splitext(img)[0]
        json_path = os.path.join(JSON_DIR, f"{char_id}.json")
        if os.path.exists(json_path):
            with open(json_path, 'r', encoding='utf-8') as f:
                j_data = json.load(f)
                display_name = f"[{j_data.get('identityName', 'ì´ë¦„ì—†ìŒ')}] {j_data.get('characterName', '')}"
        else:
            display_name = "(ë¯¸ì…ë ¥ ë°ì´í„°)"
        search_list.append({"page": i, "name": display_name})

    # ëª¨ë“  ì‘ì—… ì™„ë£Œ ì‹œ
    if page >= len(images):
        return f"""
        <div style="font-family:sans-serif; background:#121212; color:#fff; padding:3rem; text-align:center;">
            <h1 style="color:#eab308;">ğŸ‰ ì´ {len(images)}ê°œì˜ ë°ì´í„° ì‘ì—…ì´ ëª¨ë‘ ëë‚¬ìŠµë‹ˆë‹¤!</h1>
            <p>ë¹ˆí‹ˆì—†ì´ ì™„ë²½í•˜ê²Œ ë„ê°ì´ ì±„ì›Œì¡ŒìŠµë‹ˆë‹¤.</p>
            <br>
            <a href="/?page=0" style="color:#fff; text-decoration:none; padding:1rem; background:#444; border-radius:4px; margin-right:1rem;">1ë²ˆë¶€í„° ë‹¤ì‹œ ê²€í† í•˜ê¸°</a>
            <a href="/scraper" style="color:#000; text-decoration:none; padding:1rem; background:#eab308; border-radius:4px;">ìƒˆ ì´ë¯¸ì§€ ìˆ˜ì§‘í•˜ëŸ¬ ê°€ê¸° â”</a>
        </div>
        """
        
    current_image = images[page]
    char_id = os.path.splitext(current_image)[0]
    
    json_path = os.path.join(JSON_DIR, f"{char_id}.json")
    existing_data = {}
    if os.path.exists(json_path):
        with open(json_path, 'r', encoding='utf-8') as f:
            existing_data = json.load(f)
            
    # search_listë¥¼ í™”ë©´ìœ¼ë¡œ ê°™ì´ ë„˜ê²¨ì¤ë‹ˆë‹¤
    return render_template('index.html', image=current_image, char_id=char_id, page=page, total=len(images), data=existing_data, search_list=search_list)

@app.route('/save', methods=['POST'])
def save():
    char_id = request.form['char_id']
    page = int(request.form['page'])
    img_filename = request.form['img_filename']
    
    affiliation_list = [a.strip() for a in request.form['affiliation'].split(',') if a.strip()]
    checked_keywords = request.form.getlist('keywords_check')
    manual_keywords_str = request.form.get('manual_keywords', '')
    manual_keywords = [k.strip() for k in manual_keywords_str.split(',') if k.strip()]
    final_keywords = list(set(checked_keywords + manual_keywords))
    
    character_data = {
        "id": char_id,
        "characterName": request.form['characterName'],
        "identityName": request.form['identityName'],
        "isDefault": request.form.get('isDefault') == 'on',
        "grade": int(request.form['grade']),
        "releaseDate": request.form['releaseDate'],
        "imagePosition": request.form.get('imagePosition', 'center'),
        "keywords": final_keywords,
        "skills": {
            "skill1": {"type": request.form['skill1_type'], "attribute": request.form['skill1_attr']},
            "skill2": {"type": request.form['skill2_type'], "attribute": request.form['skill2_attr']},
            "skill3": {"type": request.form['skill3_type'], "attribute": request.form['skill3_attr']},
            "special1": {"type": request.form['special1_type'], "attribute": request.form['special1_attr']},
            "special2": {"type": request.form['special2_type'], "attribute": request.form['special2_attr']},
            "special3": {"type": request.form['special3_type'], "attribute": request.form['special3_attr']},
        },
        "defense": {"type": request.form['defense_type'], "attribute": request.form['defense_attr']},
        "affiliation": affiliation_list if affiliation_list else ["ë¦¼ë²„ìŠ¤ ì»´í¼ë‹ˆ"],
        "image_url": f"/images/characters/{img_filename}"
    }
    
    json_path = os.path.join(JSON_DIR, f"{char_id}.json")
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(character_data, f, ensure_ascii=False, indent=2)
        
    return redirect(url_for('index', page=page+1))

@app.route('/delete', methods=['POST'])
def delete_image():
    # í˜„ì¬ ì´ë¯¸ì§€ì™€ ê´€ë ¨ JSON íŒŒì¼ì„ ì™„ì „íˆ ì‚­ì œí•˜ëŠ” ê¸°ëŠ¥
    img_filename = request.form['img_filename']
    char_id = request.form['char_id']
    page = int(request.form['page'])
    
    img_path = os.path.join(IMAGE_DIR, img_filename)
    json_path = os.path.join(JSON_DIR, f"{char_id}.json")
    
    if os.path.exists(img_path): os.remove(img_path)
    if os.path.exists(json_path): os.remove(json_path)
    
    # ì´ë¯¸ì§€ê°€ ì‚­ì œë˜ë©´ ë’¤ì— ìˆë˜ ì´ë¯¸ì§€ê°€ í˜„ì¬ ì¸ë±ìŠ¤(page)ë¡œ ë‹¹ê²¨ì˜¤ë¯€ë¡œ ê°™ì€ pageë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
    return redirect(url_for('index', page=page))


# ==========================================
# 3. ìŠ¤í…Œì´ì§• í¬ë¡¤ëŸ¬ (ì´ë¯¸ì§€ ìˆ˜ì§‘ê¸°)
# ==========================================
@app.route('/scraper')
def scraper_ui():
    return render_template('scraper.html')

@app.route('/run_scraper', methods=['POST'])
def run_scraper():
    target_url = request.form['url']
    prefix = request.form.get('prefix', '').strip()
    if not prefix:
        prefix = "auto_img"
        
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    # ì„ì‹œ í´ë” ë¹„ìš°ê¸°
    for f in os.listdir(TEMP_DIR):
        os.remove(os.path.join(TEMP_DIR, f))
        
    # ê¸°ì¡´ ë³¸ì§„ ì´ë¯¸ì§€ í•´ì‹œ(ì§€ë¬¸) ë§¤í•‘ ì €ì¥
    existing_hashes = {}
    for f in os.listdir(IMAGE_DIR):
        if f.lower().endswith(('.jpg', '.jpeg', '.png')):
            existing_hashes[get_file_hash(os.path.join(IMAGE_DIR, f))] = f
            
    scraped_files = []
    duplicated_files = [] # ì¤‘ë³µ íŒŒì¼ ëª©ë¡
    
    try:
        response = requests.get(target_url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        img_tags = soup.find_all('img')
        
        for idx, img in enumerate(img_tags):
            img_url = img.get('src') or img.get('data-src')
            if not img_url: continue
            img_url = urljoin(target_url, img_url)
            
            # 1ì°¨ í•„í„°ë§
            if img_url.endswith(('.gif', '.svg')) or 'icon' in img_url.lower() or 'logo' in img_url.lower():
                continue

            try:
                img_data = requests.get(img_url, headers=headers, timeout=5).content
                if len(img_data) < 10240: 
                    continue
                    
                filename = f"{prefix}_{int(time.time())}_{idx}.jpg"
                filepath = os.path.join(TEMP_DIR, filename)
                
                with open(filepath, 'wb') as f:
                    f.write(img_data)
                    
                # 3ì°¨ í•„í„°ë§: ì¤‘ë³µ ê²€ì‚¬
                new_hash = get_file_hash(filepath)
                if new_hash in existing_hashes:
                    # ì¤‘ë³µì¸ ê²½ìš° í™”ë©´ì— ë³´ì—¬ì£¼ê¸° ìœ„í•´ ì •ë³´ ì¶”ê°€
                    duplicated_files.append({
                        'temp_name': filename,
                        'original_name': existing_hashes[new_hash]
                    })
                else:
                    scraped_files.append(filename)
                    
            except Exception:
                pass 
                
        return render_template('select_images.html', images=scraped_files, duplicates=duplicated_files, target_url=target_url)
        
    except Exception as e:
        return f"<h1 style='color:red;'>ì˜¤ë¥˜ ë°œìƒ</h1><p>{str(e)}</p><a href='/scraper'>ëŒì•„ê°€ê¸°</a>"

@app.route('/save_selected_images', methods=['POST'])
def save_selected_images():
    selected_images = request.form.getlist('selected_images')
    saved_count = 0
    
    for filename in selected_images:
        temp_path = os.path.join(TEMP_DIR, filename)
        final_path = os.path.join(IMAGE_DIR, filename)
        
        if os.path.exists(temp_path):
            shutil.move(temp_path, final_path)
            saved_count += 1
            
    for f in os.listdir(TEMP_DIR):
        os.remove(os.path.join(TEMP_DIR, f))
        
    return f"""
    <div style="font-family:sans-serif; background:#121212; color:#fff; padding:3rem; text-align:center;">
        <h1 style="color:#eab308;">âœ… {saved_count}ê°œì˜ ì´ë¯¸ì§€ ìµœì¢… ì €ì¥ ì™„ë£Œ!</h1>
        <p>ì„ íƒí•˜ì§€ ì•Šì€ ì°Œêº¼ê¸° íŒŒì¼ë“¤ì€ ì™„ë²½í•˜ê²Œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.</p>
        <br>
        <a href="/scraper" style="color:#fff; text-decoration:none; padding:1rem; background:#444; border-radius:4px; margin-right:1rem;">ë‹¤ë¥¸ ì‚¬ì´íŠ¸ ë” ê¸ì–´ì˜¤ê¸°</a>
        <a href="/" style="color:#000; text-decoration:none; padding:1rem; background:#eab308; border-radius:4px;">ë„ê° ë°ì´í„° ì…ë ¥í•˜ëŸ¬ ê°€ê¸° â”</a>
    </div>
    """
    
@app.route('/cleanup')
def cleanup_orphans():
    # 1. í˜„ì¬ ì¡´ì¬í•˜ëŠ” ì§„ì§œ ì´ë¯¸ì§€ë“¤ì˜ ID(ì´ë¦„) ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    valid_ids = [os.path.splitext(f)[0] for f in os.listdir(IMAGE_DIR) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    
    cleaned_count = 0
    # 2. JSON í´ë”ë¥¼ ì‹¹ ë’¤ì§‘ë‹ˆë‹¤.
    for j_file in os.listdir(JSON_DIR):
        if j_file.endswith('.json'):
            char_id = os.path.splitext(j_file)[0]
            # 3. ë§Œì•½ JSON ì´ë¦„ì´ ì§„ì§œ ì´ë¯¸ì§€ ëª©ë¡ì— ì—†ë‹¤ë©´? -> ê³ ì•„ ë°ì´í„°ì´ë¯€ë¡œ ì‚­ì œ!
            if char_id not in valid_ids:
                os.remove(os.path.join(JSON_DIR, j_file))
                cleaned_count += 1
                
    return f"""
    <div style="font-family:sans-serif; background:#121212; color:#fff; padding:3rem; text-align:center;">
        <h1 style="color:#eab308;">ğŸ§¹ ë°ì´í„° ê²€ìˆ˜ ë° ì²­ì†Œ ì™„ë£Œ!</h1>
        <p>ì´ë¯¸ì§€ê°€ ì—†ì–´ì„œ ë²„ë ¤ì§„ 'ì£¼ì¸ ìƒì€ JSON ë°ì´í„°' <b>{cleaned_count}ê°œ</b>ë¥¼ ì™„ë²½í•˜ê²Œ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.</p>
        <br>
        <a href="/" style="color:#000; text-decoration:none; padding:1rem; background:#eab308; border-radius:4px;">ë©”ì¸ìœ¼ë¡œ ëŒì•„ê°€ê¸° â”</a>
    </div>
    """

if __name__ == '__main__':
    print("ğŸš€ ë¡œì»¬ ë°ì´í„° ê´€ë¦¬ì ì„œë²„ê°€ ì¼œì¡ŒìŠµë‹ˆë‹¤! http://localhost:5000 ìœ¼ë¡œ ì ‘ì†í•˜ì„¸ìš”.")
    app.run(debug=True, port=5000)